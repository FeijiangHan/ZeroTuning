# ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training

<<<<<<< HEAD
Experience our latest version directly in Google Colab.
=======
Experience our latest version directly in Google Colab:

ðŸ”— [Run on Google Colab](https://drive.google.com/file/d/12eBEkau-OGalUPipfd_fRI8M-twpmLIc/view?usp=sharing)
>>>>>>> 53d3d5111b7f8e66cea4d72de6730959210ec375

## Quick Start

### Prerequisites
- Python 3.8+
- transformers==4.53.2 (This specific version is required)

### Installation

1. Download our core components using gdown:
```bash
# Download our modified LLama model
!gdown --id 1JYr9Do94hfzc91NyxKBSJCwsTNw5ygK6

# Download evaluation pipeline
!gdown --id 17PhF8wGp9X5puN_kr0WzW7r5_ynlShXs

# Download evaluation configuration file
!gdown --id 1ZNbNV_ePNckVuNbkzQjJAqJoODy-GSW8

# Download and extract dataset
!rm -r ./data
!mkdir data
!gdown --id 1LjcvWQ84JqZQKMQrsxxIVnv0uug8hKWP -O data.zip
!unzip data.zip -d data
```

2. Export the ipynb and open the notebook using the Colab 
3. Follow the step-by-step cells to test the model
4. (Optional) Experiment with your own inputs or datasets by modifying the code

---

## Abstract

Training-free methods for enhancing large language models (LLMs) have garnered increasing attention, with token-level attention tuning emerging as an interpretable and promising direction. While existing methods typically rely on auxiliary mechanisms to identify task-specific tokens, introducing potential bias and limiting applicability, we present a novel approach.

Our research uncovers a remarkable finding: the semantically empty initial token (e.g., <BOS> in Llama) serves as a powerful and underutilized control point for optimizing model behavior. Through theoretical analysis, we demonstrate that tuning the initial token's attention dynamically adjusts the attention distribution over subsequent tokens, with its role as an attention sink amplifying this effect. Our empirical findings reveal that: (1) tuning its attention surpasses the effectiveness of tuning other task-specific tokens in improving LLM performance across tasks; (2) the effect demonstrates a consistent pattern across layers, with earlier layers showing greater impact, while varying across attention heads, each displaying distinct preferences in their attention to this token.

Building on these insights, we introduce **ZeroTuning**, a training-free approach that enhances LLM performance through head-specific attention adjustments to this special token. Despite focusing on a single token, ZeroTuning achieves superior average performance across text classification, multiple-choice QA, and multi-turn conversation tasks on models including LLama, Qwen, and DeepSeek. Notable improvements include an 11.71% boost for Llama-3.1-8B on classification tasks, a 2.64% increase on QA tasks, and an enhancement in multi-turn score from 7.804 to 7.966. The method demonstrates robust performance across limited resources, few-shot settings, long contexts, quantization, various decoding strategies, and prompt variations.

Our work illuminates a previously overlooked control point in LLMs, offering valuable insights into both inference-time tuning and model interpretability.
